{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTVb8J9NFUfI"
   },
   "source": [
    "# Murderpedia Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxQIaDaDeoxK"
   },
   "source": [
    "* Author: [Daniel Iova](https://github.com/daniel-iova)\n",
    "* Github Repo: [murderpedia-web-scraper](https://github.com/daniel-iova/murderpedia-web-scraper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Base components](#Base-components)\n",
    "3. [Getting started](#Getting-started)\n",
    "4. [Prerequisite methods](#Prerequisite-methods)\n",
    "5. [Defining the brains of the operation](#Defining-the-brains-of-the-operation)\n",
    "6. [Collecting, cleaning and storing the data](#Collecting,-cleaning-and-storing-the-data)\n",
    "6. [Conclusions](#Conclusions)\n",
    "7. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IALBfEpHdx0e"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbZEIbXFd0Jr"
   },
   "source": [
    "The goal of this project, along with introducing me to the world of data science, is to scrape data from *most* entries (murderers) on [Murderpedia](https://murderpedia.org).\n",
    "\n",
    "Each murderer stored on the website has a table of *interesting data*, which is what I want to collect.\n",
    "<br>This table contains keys like \"Classification\", \"Location\", \"Status\", etc.\n",
    "\n",
    "All collected data is stored as a *json serialized object* and can easily be integrated into a non-relational database.\n",
    "<br>For those that need to store the data in a relational database, a *general table model* is also created.\n",
    "\n",
    "With that being said, the scraper can be extended to collect all data present on each murderer's page, but that is out of the scope of this project.\n",
    "\n",
    "Due to :\n",
    "- the inconsistencies between entries\n",
    "- the variable way the pages were written\n",
    "- the way http requests work<br>\n",
    "\n",
    "the scraper often produces different results with each run (max 10 entries difference between runs).\n",
    "\n",
    "Even so, the number of returned entries is around 85-95% of the original supposed 6921 entries, which is still a huge amount of data.\n",
    "<br>However, due to how large this dataset is, the scraper is relatively slow (execution time of ~15-25 minutes on a modern computer).\n",
    "\n",
    "The scraper produces three files:\n",
    "* **dataset.json** : contains the data that could be retrieved.\n",
    "* **model.json** : contains a dictionary with all keys that appear in the dataset and their occurances.\n",
    "* **count.txt** : contains the number of entries that were scraped.\n",
    "\n",
    "The *License*, along with the source files for this project, are found on the github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJjqv0h5YYPH"
   },
   "source": [
    "## Base components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI-bZw-Mjd37"
   },
   "source": [
    "The two most important components of this web scraper are:\n",
    "* The **process_all_murderers** method which is the brain of the scraping operation.\n",
    "* The **global_dataset** dictionary that stores the results of the above method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK40I_QwXwed"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPKUVi3A6GUJ"
   },
   "source": [
    "First of all we need to install all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdw8ijcM6GEU"
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "!pip install bs4\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_YbTklDYk4z"
   },
   "source": [
    "We then import the libraries needed to run the scraper..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uCNNYmY0wno"
   },
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvUd0qAM429R"
   },
   "source": [
    "...and instantiate the **global_dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GEO5rfu41W7"
   },
   "outputs": [],
   "source": [
    "global_dataset = defaultdict(lambda: defaultdict(dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY0AhzaWztfS"
   },
   "source": [
    "## Prerequisite methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_pJQM6509sj"
   },
   "source": [
    "We need to define methods to tackle all aspects of scraping the data for a murderer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek9WLssr1Pys"
   },
   "source": [
    "The **get_page_lang** method returns the page language.\n",
    "<br>We only want to scrape English pages, so we need a way to break the process if the page is in a foreign language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnwVaN7s0nsv"
   },
   "outputs": [],
   "source": [
    "def get_page_lang(soup):\n",
    "    lang = str(soup.find(\"meta\", attrs= {\"http-equiv\":\"Content-Language\"}))\n",
    "    lang = lang[lang.find(\"content=\") + 9 : lang.find(\"content=\") + 11]\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kCgrJr51mnv"
   },
   "source": [
    "<br>The **get_image_url** method is created to find the image url of our entry.<br>Fuzzywuzzy's *token_set_ratio* method is used to compute the similarity between the found image's filename and the murderer's link.\n",
    "<br>If the ratio is above a certain threshold (25 in our case), we return the image url. If not, we return \"IMG_NOT_FOUND\".\n",
    "<br>Due to this, the method has an accuracy of ~ 90%.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6M96MSn0qW1"
   },
   "outputs": [],
   "source": [
    "def get_image_url(soup, murderer):\n",
    "    img_urls = soup.find_all(\"img\")\n",
    "    for img in img_urls:\n",
    "        if \"../images/\" in img[\"src\"]:\n",
    "            tokens = re.split(\"/\", img[\"src\"], re.UNICODE)\n",
    "            ratio = fuzz.token_set_ratio(tokens[-2], re.sub(r\"\\d+\", \"\", tokens[-1]))\n",
    "            if (ratio > 25):\n",
    "                img_url = murderer[\"Base_URL\"] + img[\"src\"][3:]\n",
    "                return img_url\n",
    "    return \"IMG_NOT_FOUND\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt0jYMet26eQ"
   },
   "source": [
    "<br>The **get_tables** method is defined for the purpose of finding the list of \"table\" tags that *could* contain the data we want to extract.\n",
    "<br>Along with the soup object, the function receives a *font_size* parameter, because the table's styling either has a font size of 8 or a font size of 10.<br>First we run with 8, and, if no tables were found, we try with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miY7vNAw05Xz"
   },
   "outputs": [],
   "source": [
    "def get_tables(soup, font_size = 8):\n",
    "    return soup.find_all(name = \"table\", \n",
    "                           attrs = {\"border\" : \"0\",\n",
    "                                    \"style\" : f\"font-size: {font_size}pt; color: #000000; border-collapse: collapse\", \n",
    "                                    \"cellpadding\":\"0\" }), font_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtpIwNeO4Dk_"
   },
   "source": [
    "<br>We define the **process_text** method to deal with the irregular formatting of the text we extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yotmlotg3zJr"
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EpT_DIyXNZM"
   },
   "source": [
    "<br>The **get_dict_from_data_list** method converts a list of *\"raw_key:raw_value\"* strings to a dictionary and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du_tQh_VXCrY"
   },
   "outputs": [],
   "source": [
    "def get_dict_from_data_list(data_list):\n",
    "    data_dict = {}\n",
    "    for item in data_list:\n",
    "        item = re.sub(r\"\\:+\", \":\", item)\n",
    "        split = re.split(r\"\\:\", item)\n",
    "        data_dict[unidecode(process_text(split[0]))] = unidecode(process_text(split[1]))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f47UVImU4-pI"
   },
   "source": [
    "<br>All above methods are used by the **get_data** method.\n",
    "<br>This method executes the following steps:\n",
    "1. Initialize an empty *data dictionary*.\n",
    "2. Get the soup object associated to the given murderer.\n",
    "3. Use **get_page_lang** to break the processing if the page is not in English.\n",
    "4. Assign the return value of **get_image_url** to the *Image_URL* key in the *data dictionary*.\n",
    "5. Use **get_tables** to get the particular \"table\" tags and the *font size*.\n",
    "6. Build a *data list* by iterating through the tables and finding specific \"td\" tags in the current table with the help of the *font size*.<br>Stop iteration after we find \"Status:\" in one of the \"td\" tag's text.\n",
    "7. Use **get_data_dict_from_list** to update the *data dictionary* with data associated to the given murderer.\n",
    "8. Return the *data dictionary*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcqQh8xy4wpg"
   },
   "outputs": [],
   "source": [
    "def get_data(murderer):\n",
    "    \n",
    "    data_dict = {}\n",
    "\n",
    "    url = murderer[\"Murderpedia_URL\"]\n",
    "    r = requests.get(url, headers = {\"User-Agent\": 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'})\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html5lib\")\n",
    "\n",
    "    if get_page_lang(soup) != \"en\":\n",
    "        return None\n",
    "\n",
    "    data_dict[\"Image_URL\"] = get_image_url(soup, murderer)\n",
    "\n",
    "    tables, font_size = get_tables(soup)\n",
    "    if tables == []:\n",
    "        tables, font_size = get_tables(soup, 10)\n",
    "    \n",
    "    for t in tables:\n",
    "        tds = t.find_all(name = \"td\", attrs = {\"width\":\"100%\", \"style\": f\"font-size: {font_size}pt; color: #000000\"})\n",
    "        data_list = []\n",
    "        i = 0\n",
    "        ok = 0\n",
    "        classif_index = 0\n",
    "        for td in tds:\n",
    "            data_list.append(process_text(td.text))\n",
    "            if \"Classification:\" in data_list[i]:\n",
    "                classif_index = i\n",
    "            i+= 1\n",
    "            if \"Status:\" in td.text:\n",
    "                ok = 1\n",
    "                break\n",
    "        if ok == 1:\n",
    "            break\n",
    "    data_list = data_list[classif_index:]\n",
    "\n",
    "    data_dict.update(get_dict_from_data_list(data_list))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gdc94GMKwxi"
   },
   "source": [
    "<br>The **process_murderer** method builds the data dictionary for the murderer using the given parameters and the **get_data** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwHopuq0KuDM"
   },
   "outputs": [],
   "source": [
    "def process_murderer(gender, letter, murderer, index):\n",
    "    data = {}\n",
    "    data[\"Name\"] = murderer[\"Name\"]\n",
    "    data[\"Murderpedia_URL\"] = murderer[\"Murderpedia_URL\"]\n",
    "    data.update(get_data(murderer))\n",
    "    return gender, letter, data, index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I-6cm5t1L33"
   },
   "source": [
    "<br>The **name_to_key** method is used to generate the key where the data is stored in the **global_dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_0qwyPs1JNy"
   },
   "outputs": [],
   "source": [
    "def name_to_key(name, index):\n",
    "    return re.sub(r\"[^\\w]\", \"\", name) + f\"_{index}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkl6hzPRL2LR"
   },
   "source": [
    "<br>The **add_to_global_tree** method uses the return tuple of the **process_murderer** method to add an entry to the **global_dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PdE66PSLrSj"
   },
   "outputs": [],
   "source": [
    "def add_to_global_tree(gender, letter, dic, index):\n",
    "    global_dataset[gender][letter][name_to_key(dic[\"Name\"], index)] = dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSEudUzJr5d2"
   },
   "source": [
    "<br>The **single_entry** method is defined to facilitate multithreading.\n",
    "<br>It is responsible with updating the **global_dataset** with an entry for the given murderer.\n",
    "<br>It receives a list named *components*, which has the following elements:\n",
    "\n",
    "1. Tag that contains the link to the murderer\n",
    "2. Base murderer dictionary\n",
    "3. Gender\n",
    "4. Letter\n",
    "5. Index (used to differentiate between murderers with the same name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTnLrViEr58z"
   },
   "outputs": [],
   "source": [
    "def single_entry(components):\n",
    "    x, dic, gender, l, index = components\n",
    "    base_url = \"http://murderpedia.org/\" + gender + \".\" + l+\"/\"\n",
    "    dic[\"Murderpedia_URL\"] = base_url + x[\"href\"]\n",
    "    dic[\"Base_URL\"] = base_url\n",
    "    try:\n",
    "        add_to_global_tree(*process_murderer(gender, l, dic, index))\n",
    "        print(\"DONE\", dic[\"Name\"])\n",
    "    except:\n",
    "        print(\"COULDN'T DO\", dic[\"Name\"], dic[\"Murderpedia_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnKdbIrZE-9G"
   },
   "source": [
    "## Defining the brains of the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPPfcxSzqDGw"
   },
   "source": [
    "<br>The **process_all_murderers** method is defined for the purpose of getting all scrapable data and assigning it to the **global_dataset**.\n",
    "<br>Because the murderers are stored with respect to the first letter of their surname, we first scrape the pages associated to the letters.\n",
    "<br>For each \"letter page\" we define a list where we append *components* used by the **single_entry** method.\n",
    "<br>After all possible entries were appended, we pass it to a *ThreadPoolExecutor*'s map method.\n",
    "<br>The number of worker threads is given as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnUnjy-zElYK"
   },
   "outputs": [],
   "source": [
    "def process_all_murderers(workers):\n",
    "    \n",
    "    genders = [\"male\", \"female\"]\n",
    "    alphabet = list(string.ascii_uppercase)\n",
    "\n",
    "    for gender in genders:\n",
    "        base = \"http://murderpedia.org/\" + gender\n",
    "        for letter in alphabet:\n",
    "            \n",
    "            # Build url and get html content\n",
    "\n",
    "            url = f\"{base}.{letter}/index.{letter}.htm\"\n",
    "            r = requests.get(url, headers = {\"User-Agent\": 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'})\n",
    "            c = r.content\n",
    "            soup = BeautifulSoup(c, \"html5lib\")\n",
    "\n",
    "            # Build a list of all correct <tr> rows\n",
    "\n",
    "            allrows = soup.findAll(\"tr\")\n",
    "            rows = [row for row in allrows if row.text.strip() != \"\"][13:]\n",
    "            \n",
    "            # Build the entry_thread_list used to facilitate multithreading\n",
    "\n",
    "            entry_thread_list = []\n",
    "            index = 0\n",
    "            for row in rows:\n",
    "                data = row.findAll(\"td\")\n",
    "                links  = row.findAll(\"a\")\n",
    "                if len(data) == 5:\n",
    "                    dic = {}\n",
    "                    dic[\"Name\"] = re.sub(\"&\", \"and\", re.sub(r\"\\s+\", \" \", data[1].text.strip().replace(\"\\n\",\"\").replace(\"\\t\",\" \")))\n",
    "                    if dic[\"Name\"] != \"\":\n",
    "                        for x in links:\n",
    "                            if re.match(r\"%s\\d*/.*\" % letter, x[\"href\"], re.I):\n",
    "                                entry_thread_list.append([x, dic, gender, letter, index])\n",
    "                                index += 1\n",
    "                                break\n",
    "                    else:\n",
    "                        print(f\"DIDN'T PASS NAME CHECK\" + dic[\"Name\"])\n",
    "\n",
    "            # Execute the calls to the single_entry method using the entry_thread_list\n",
    "            \n",
    "            with cf.ThreadPoolExecutor(max_workers=workers) as executor : \n",
    "                executor.map(single_entry, entry_thread_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToUi24S7v9JY"
   },
   "source": [
    "## Collecting, cleaning and storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CWoVfYpwFEF"
   },
   "source": [
    "<br>Now that we defined the base functions, we can go ahead and actually use them.<br>\n",
    "<br>First, we define where we want to save the data we collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTB1-dWcwJ1h"
   },
   "outputs": [],
   "source": [
    "    dataset_filename = \"dataset.json\"\n",
    "    model_filename = \"model.json\"\n",
    "    count_filename = \"count.txt\""
   ]
  },
  {
   "source": [
    "<br>Then, we define the number of workers we want to use.\n",
    "<br>I found that 8 workers offers a great balance between speed and number of different lost entries between runs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bfQJLtEwNHM"
   },
   "source": [
    "<br>Finally, to get the raw data, we call the **process_all_murderers** method.\n",
    "<br>**Disclaimer**: The run time of this method is between **15 and 25 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga0Mnsv5wU-7",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "process_all_murderers(max_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1GO-udVwXc4"
   },
   "source": [
    "<br>The data needs to be cleaned, so we define the **clean_data** method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taLxd4zAxHSz"
   },
   "outputs": [],
   "source": [
    "def clean_data(dataset):\n",
    "    for g in list(dataset.keys()):\n",
    "        for l in list(dataset[g].keys()):\n",
    "            for m in list(dataset[g][l].keys()):\n",
    "                if len(dataset[g][l][m]) <= 5:\n",
    "                    del dataset[g][l][m]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFJpjiRIluwn"
   },
   "source": [
    "...and use it to clean the dataset. We dump the resulting dictionary to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KA80U9hAl1bK"
   },
   "outputs": [],
   "source": [
    "dataset = clean_data(global_dataset)\n",
    "json.dump(dataset, open(f\"{dataset_filename}\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f07EZ4vxupD"
   },
   "source": [
    "<br>After this, we compile a *general table model*.<br>\n",
    "It contains an instance of every key of all scraped murderers, each key having it's no. of occurances in the dataset as value.\n",
    "<br>We can later use this model if we want to store the data into a relational database.\n",
    "<br>This is done by defining and using the **compile_general_model** method, which receives our dataset and a filename for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IodEyGf7yN6N"
   },
   "outputs": [],
   "source": [
    "def compile_general_model(dataset, model_filename):\n",
    "    occurances = {}\n",
    "    for gender in dataset.keys():\n",
    "        for letter in dataset[gender].keys():\n",
    "            for murderer in dataset[gender][letter].keys():\n",
    "                for key in dataset[gender][letter][murderer].keys():\n",
    "                    if key not in occurances.keys():\n",
    "                        occurances[key] = 1\n",
    "                    else:\n",
    "                         occurances[key] += 1\n",
    "    json.dump(dict(sorted(occurances.items(), key= lambda x : x[1], reverse=True)), open(model_filename, \"w\"))\n",
    "\n",
    "\n",
    "compile_general_model(dataset, f\"{model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS95cKOXy6ma"
   },
   "source": [
    "<br>In the end, we count the number of murderers we scraped.\n",
    "<br>This is done by defining two methods: \n",
    "* **count_children** : counts the number of children on only one branch, \"male\" or \"female\".\n",
    "* **count_murderers** : counts all murderers using the first method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdGNQ7dNy_BJ"
   },
   "outputs": [],
   "source": [
    "def count_children(dic):\n",
    "    count = 0\n",
    "    for key in dic.keys():\n",
    "        count += len(dic[key])\n",
    "    return count\n",
    "\n",
    "def count_murderers(dataset_filename, count_filename):\n",
    "    outf = open(count_filename, \"w\")\n",
    "    data = json.load(open(dataset_filename))\n",
    "    male_count, female_count = 0, 0\n",
    "    if \"female\" in data.keys():\n",
    "        female_count = count_children(data[\"female\"])\n",
    "    if \"male\" in data.keys():\n",
    "        male_count = count_children(data[\"male\"])\n",
    "    print (f\"{male_count} Males and {female_count} Females.\", file = outf)\n",
    "    print (f\"In total {male_count+female_count} out of 6921 total entries.\", file = outf)\n",
    "    print (f\"{6921 - (male_count+female_count)} murderers not scraped.\", file = outf)\n",
    "\n",
    "\n",
    "count_murderers(f\"{dataset_filename}\", count_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMyYTouPchos"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLBl364zckQT"
   },
   "source": [
    "<br>As my first project, this was a great learning experience.\n",
    "<br>It taught me all about how http requests work, how data is scraped from websites, multithreading, and so on.\n",
    "<br>Along with this, it allowed me to collect great deals of data, which will be used in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Murderpedia](murderpedia.org) by [Juan Ignacio Blanco](https://es.wikipedia.org/wiki/Juan_Ignacio_Blanco)\n",
    "* [Murder...and learning how to analyze data](https://mellybess.github.io/2017/01/23/getting-some-data.html) by [mellybess](https://github.com/mellybess)\n",
    "* [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNksUkefWJ2FkrtGs27ZHR5",
   "collapsed_sections": [],
   "name": "murderpedia-web-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}